{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a803038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79eaba2",
   "metadata": {},
   "source": [
    "We import pandas as a way to manipulate our data. Pandas uses sqlalchemy to interact with relational databases, so that is needed to create connection and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac496389",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=100)\n",
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa7462",
   "metadata": {},
   "source": [
    "This instruction read the first 100 lines from our dataset (a .csv file).\n",
    "The last to lines basically tell pandas to translate the dates in the SQL date type instead of normal text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6111ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "print(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ce400",
   "metadata": {},
   "source": [
    "We create the connection to postgres (note that the string is in this format *type_of_connection://username:password@host:port/database*).\n",
    "The second part creates the database schema and prints it. The table will have yellow_taxi_data as a name and uses the df variable previously set to populate the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365776a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = pd.read_csv('yellow_tripdata_2021-01.csv', iterator=True, chunksize=100000)\n",
    "df = next(df_iter)\n",
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "\n",
    "df.head(n = 0).to_sql(name='yellow_taxi_data', con = engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b238d84",
   "metadata": {},
   "source": [
    "Inserting millions of rows of data at one time can create many problems. What we need to do is to **batch** (basically divide) the problem in smaller ones.\n",
    "To do that, we can use pandas **iterators**. These work like normal iterators, the first time we run the .to_sql only to recreate the schema (if_exist='replace' and head(n=0) means that we dont actually insert data inside the table and just recreate the schema from scratch, dropping the previous one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea7a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.to_sql(name='yellow_taxi_data', con = engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b82ee",
   "metadata": {},
   "source": [
    "This is one way to insert data (note that we use if_exists='appen' so that we dont drop the table but just add data), but we are not taking advantage of our iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8faed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "while True:\n",
    "    t_start = time()\n",
    "    \n",
    "    df = next(df_iter)\n",
    "\n",
    "    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "\n",
    "    df.to_sql(name='yellow_taxi_data', con = engine, if_exists='append')\n",
    "\n",
    "    t_end = time()\n",
    "\n",
    "    print ('inserted another chunk, took %3.f second' % (t_end - t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b117a",
   "metadata": {},
   "source": [
    "This, while not being the best and most elegant looking code, is a way to insert every last piece of data in our dataset (in chuncks of chunksize=100000 as previously stated).\n",
    "We basically iter until df = next(df_iter) throws an Exception and forcefully closes the code snippet, in fact ***next***(*iterator*) returns the next element of the iterator if present, and an Exception if the iterator is empty.\n",
    "In the future we will see ways to manage this operations more safely and efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
